\newpage

## ソースベース解析の理屈
かなり面倒いので丁寧にはかけません。ここからは線形代数の範囲になります。
分かる人に言うと、疑似逆行列を正規化した方法がMNEです。

以下、二行目で「は？」ってなったそこの君、PRMLを読んで下さい。
PRMLを読めないそこの君。君のために解説を書きます。
誤解を恐れずに脳内の活動を推定する方法を超簡単に言いますと、
**ソース推定とは鶴亀算であり、割り算です。**
「鶴亀算と割り算は別のものじゃんｗｗｗ」と思った人は頑張らねばなりません。
なので、そういう人のために、割り算については後述します。

さて、MNEはそんなふうに割り算をしてあげた上で、答えをもうちょっと
スムーズにしてあげる為に正規化という処理を施して上げるものです。

つまり、**MNEとはスムージング入りの割り算である！**
と初心者には伝えたいです(怒られそう…)
数学得意な人のために一言で言うと下記の表のとおりです。

| Method       | 内容                                     |
|--------------|------------------------------------------|
| MNE          | 脳全体のノルム関する重み付き最小ノルム解 |
| dSPM/sLORETA | 分散で補正したMNE                        |
| eLORETA      | 事前確率分布を事後確率分布で補正するMNE  |
| beamformer法 | 波のノルム関する重み付き最小ノルム解     |

### もう一寸ちゃんと

まず、脳のソースから出た磁力…電力でも良いですが、そのようなものがセンサーに届く時、
その強さはソースで発生した電力、磁力に比例するはずです。
詳細はアレですね…マクスウェルの方程式ですね。多分、距離の二乗には反比例すると思います。
つまり…距離云々を除くと簡単な一次連立方程式になるはずなんです。
センサーで捉えた情報を$y$とし、ソースで発生したのを$x$としましょう。
あるソースで発生した電力とセンサーで捉えた電力の関係を下記の式で表せるとします。
$$y=ax$$
これは単純な掛け算なわけですが、1センサー1ソースではなく、多センサー多ソースです。
ここで、沢山になったy,a,xについて、下記のように表すとします。
$$Y={y_1,y_2,y_3......}$$
$$X={x_1,x_2,x_3......}$$
ここで$A$を下記を満たす行列とします。
$$Y=AX$$
この連立方程式を解きます。

横と縦を掛け算するので、行列の掛け算は連立方程式になるのは自明です。
これがどうしても連立方程式に見えない人は、
行列代数の入門書でも読んで下さい。ネットで検索するよりこれは本の方がいいです。

では解きま…じつは解けません。一次連立方程式はあんまり数が多くなると解けなくなるのです。
鶴亀カブトムシ算は無限の解があるのです。

で、最も真実に近いっぽいのを推定していきます。
今回、脳全体のある一瞬の波が最も小さくなるような波を推定しましょう。
何故最小にしたいかって？乱れた波というものはグニョングニョンしてるので
直感的に言ってグニョングニョンしてる波よりも静かな波のほうが
本物っぽいでしょう？と位にしか言えません！
その本物っぽい割り算を$A^{\dagger}$として
$$X \Rightarrow A^{\dagger}Y$$
という感じにするのを目指します。

この方法がMinimumNormEstimation(MNE)です。
(他に一つ一つの波が一番小さくなるbeamformer法とか色々あります)

条件$Y=AX$のもとで$X$を小さくしたい[^norm]のです。
$X$が小さいってことは下記の式が小さいってことです。

$$||X||^2$$

ここで、$||X||$とはXの対角成分の事を指します。
例えばXの縦をチャンネル、横を時間軸とします。
${||X||}^2$がどうなるかと言うと、$||X^TX||$となります。
何故これの対角行列かと言うと、
Xの1行目…例えばチャンネル1としましょう。これとXの2列目…これはチャンネル2です。
これらの掛け算はチャンネル間の相関を表しているのであって、振幅を表しません。
つまり、対角行列のみが大きさを表すのです。

[^norm]: ノルムが小さいということ。中学生風に言うと絶対値。

まず、ラグランジュの未定乗数法という公式を使います。知らないカスはググれ。
僕はこんな同人誌を書く程度には親切で友達がいっぱい出来そうな人[^tomodati]なので教えてあげます。
最大値や最小値を手軽に導くテクニックです。
最大値や最小値は極値になるので、微分して0になればいいのです。

[^tomodati]: リアル友達欲しいです(´;ω;｀)

$f(x,y)$が極値になるx,yは

$$L(x,y,\lambda)=f(x,y)-\lambda g(x,y)$$
の時に

$$\frac{\partial L}{\partial \lambda}= \frac{\partial L}{\partial y}= \frac{\partial L}{\partial x}=0$$
の解、または
$$\frac{\partial g}{\partial x}= \frac{\partial g}{\partial y}$$
の解。…という感じの公式です。
行列の微分をしないといけないので、行列の微分の仕方を確認しておきます。
$$\frac{\partial a^tx}{\partial x}=a$$
$$\frac{\partial x^ta}{\partial x}=a$$

今回はこれを微分します。
$$L=\frac{X^2}{2}-\lambda^T (AX-Y)$$

$$\frac{\partial L}{\partial X} = X - A^T\lambda$$
これが0になるので
$$X = A^T\lambda $$
$$Y = AX = A\lambda A^T = AA^T\lambda$$
$$X = A^T(AA^T)^{-1}Y$$

これで無事$X$を$A$と$Y$で表せました。
まぁ、ここまでは普通の最小二乗の最適解なのだけれど。

### もっと良くする
もっと良くします。

$$Y = AX$$

という感じの割り算をときます。
が、今回はL2ノルムを使います。L2ノルムについてはググって下さい。
L2ノルムは要するに二乗をちっさくしようと言うことです。

$$argmin L_{x} =  {||Y - AX||}^2 + \lambda {||WX||}^2$$

ここで、WXとは何かという疑問が生じます。
XはまぁXなんですが、WXは「これがちっさくなって欲しいな？」というものです。
WがIでも計算は出来るのですが、WがIの場合どうなるかと言うと
「なるべく全てが、平等に正則化されるように」です。
Wは重みとでも言えるもので、Xの振幅の逆数とでも言えるようなものです。
つまり、Xが小さくなってほしいと言うより、
Xの大きい部分は大きく、小さい部分は小さく予測したいのです。
$W^TW$の逆数は分散になるような感じです。
詳しくは最小二乗法でググれ。誰かが「重み付き最小二乗法」とかで書いてる。

ここで、$||X||$はXのトレースなのですが、行列の定義をみて計算してみて下さい。
これはきちんと「Xの最小値」を求める式になっています。
$$ \hat L= tr((Y^T - X^TA^T)(Y - AX)) + \lambda tr(X^TW^TWX)$$
$$=tr(Y^TY - X^TA^TY - Y^TAX + X^TA^TAX) + \lambda tr(X^TW^TWX)$$
ここで、微分します。
$$\frac{\partial \hat L}{\partial X} = - 2A^TY + 2 A^TAX + 2 \lambda W^TWX= 0$$

$$-A^TY + A^TAX + \lambda W^TWX = 0$$
$$X = (A^TA + \lambda W^TW)^{-1}A^TY$$
さて、$W^TW$は分散の逆数なのですが、
$(W^TW)^{-1}$が長いので$\Sigma$って書き換えさせて下さい。
$$X = (A^TA + \lambda \Sigma ^{-1})^{-1}A^TY$$
出ました！万歳！MNEの式です！だけど、なんだか書いてあるのと違う？

お前は教科書に載っている通りじゃないと満足しない心の狭い人間なのだね？
OK、書き直すわ…
$$(A^TA + \lambda \Sigma)A^T = \Sigma A^T(A\Sigma A^T + \lambda I)^{-1}$$
上の式をじっと見て下さい。成立することが解ると思います。
故に、こうなります。

$$X = \Sigma A^T(A\Sigma A^T + \lambda I)^{-1}Y$$
これやこの、MNEの式です。
だけど、まだ$\lambda I$が怪しいですね？ここで、はじめの式を見て下さい。
$$argmin L_{x} =  {||Y - AX||}^2 + \lambda {||WX||}^2$$
これ、あからさまに第一項がYにまつわるもの、第二項がXにまつわるものですね？
ということは、XにWがあるならYにもそれに相当するものがあって良いはずです。
だから、脳内のノイズの分散共分散行列をCとしてあげます。
…ノイズと言いましたが「背景」と言ったほうがわかりやすいかも知れません。
数学得意な人は「重み付き最小ノルム解」と言います。
数学得意な人は一々強そうな名前言いますよね…。式はこんな感じ。
$$argmin L_{x} =  C{||Y - AX||}^2 + \lambda {||WX||}^2$$

実際計算するとこうなるようです。
$$X = \Sigma A^T(A\Sigma A^T + C)^{-1}Y$$
で、この値は正則化されていないわけですから、
グニョングニョンの値になってしまいます。
なので、結局正則化のパラメータ入れるのです...。
(ここは流石にググって)

$$X = \Sigma A^T(A\Sigma A^T + \lambda^2 C)^{-1}Y$$
読者の皆さんはつかれましたか？僕も疲れました。

まだ続きます(´・ω・｀)

### 白色化
脳内の各皮質って、それぞれ独立でいて欲しいですよね？
じゃないと、脳内に沢山点々を打った意味がなくなります。
こういう風に、それぞれの相関を打ち消す操作を白色化と言います。
まず、想定されるノイズに関して、分散共分散行列を作ります。

ぐぐｒ…これは、分散を集めた的な行列です。
これの対角成分は波そのものの分散なはずです。
毎回観測される波は同じようなものであるという想定のもとで推定をしたいので、
この分散共分散行列を式に入れ込みます。

$$Y = AC^{1/2}C^{-1/2}X$$
$$\hat A^{\dagger}Y = C^{-1/2}X$$
$$\hat A = A^{\dagger}C^{1/2}$$
の元で計算できる$A^\dagger$を$\hat {A^\dagger}$とすると
$$ \hat{A^{\dagger}}C^{1/2}Y = X$$
こんなかんじ。
$$ \hat{A^{\dagger}}C^{1/2} $$
$$ = \Sigma A^TC^{1/2}(AC^{1/2}\Sigma C^{1/2}A^T + C)^{-1}C^{1/2}$$
$$ = \Sigma A^T(A\Sigma A^T + \lambda ^2 I)^{-1}Y$$
せっかくここまでCを入れてたのに脱力感半端ないっすね。
まぁ、実際の式ではCを掛け算したりはしますが、逆行列からは外れます。

### MAP推定
今回はラグランジュの未定乗数法で二乗したやり方を書きましたが、
ここまででも書きましたが、ベイズ統計学の視点からの見方もあります。
しかし、ここまで書いて疲れました。これはここに書くのが超絶面倒いので書きません。

これの解説だけで本が一冊書けます。ぷるむるでも読めば良いんじゃないかな？

### dSPMの理屈
さて…MNEの式をよく眺めてみましょう。これは
$$Y=AX$$
の変形であり、シンプルな掛け算なのは言うまでもありません。
そこで、MNEの式を次のように書き直してみます。
$$X=A^\dagger Y$$

ここで、ふとした疑問が出てきます。
$A^\dagger$の分散が1じゃない場合に奇妙なことが起こります。
$A^\dagger$が1じゃない場合で、空室を撮ったと仮定してみて下さい。
センサーが捉えるノイズと空室の分散は同じ大きさのはずですが、
$A^\dagger$が1じゃなかったら空室の中のノイズが
大きくなったり小さくなったりしておかしいですね？？？
脳が存在しない場合でも、脳があるはずの所に何かが生じてしまう。

ということで、計算結果を補正してあげる必要があります。
通常、補正の方法は分散の1/2乗で割り算してあげる事によって為します。
分散って言うと、振幅の二乗ですから、つまり振幅で割ってる的なことです。
それぞれのソースを、そのソースの振幅で割り算してあげるのです。

振幅で割るってどうするのかって？
対角成分で割り算してあげるのです。

答えを分散で割るとき、その方法が有名所が2つあります。
一つの方法は下記です。

$$Y = AX$$
$$A^{\dagger}Y = X$$
この$A^\dagger$を補正してあげるのですが、ソースの振幅を出さねばなりません。
センサーの振幅は分散の1/2乗なので$C^{1/2}$とします。
そこで、ソースの対角成分を考えます。Yの誤差をyとしておきましょう。
CはYの分散、即ちyの二乗に比例します。
そして、縦横ともにYのチャンネル数に等しいです。
$||A^\dagger y||$の大きさ的な物で割り算したいんですよね？
なら、チャンネル数じゃなくてyのソース数分の大きさの行列にするべきです。
$A^\dagger$は縦がチャンネル数、横がソース数の行列になるはずです。
その、縦横を考慮して、ソースを割り算できる行列を作るなら、
$||A^\dagger y (yA^\dagger)^T||$ と言う感じの形になるはずです。

つまり
$$\sqrt{||A^\dagger C(A^\dagger)^T||^2} $$
となります。これで割るわけです。だから、こうなります。

$$X =||A^\dagger C(A^\dagger)^T||^{-1/2} A^\dagger Y $$
これぞ、dSPMです！

### sLORETAの理屈
ソースの分散を別の方法で計算してみます。
$$\sqrt{||X^TX||} \Rightarrow \sqrt{||Y^TA\dagger X||} = \sqrt{||Y^TA^\dagger AY||}$$
つまり、推定された世界ではXの分散は$||A^\dagger A||$倍になっているということです。
じゃ、それで割ればいいということになりますね。

$$X = ||A^\dagger A||^{-1/2}A^\dagger Y $$
はい、sLORETAです。

### eLORETAの理屈
もうほんとに疲れてきましたが、ここが最後かな…。

sLORETAは$A^\dagger A$で割り算していました。
だけど、本当に$A^\dagger A$でいいん？という問題があります。
実はこれ、もっとよく出来るよ！というのがeLORETAです。
やることは、上記を再帰的にしたものです。

発想としては、この式の$\Sigma$ってどうよ？ってことです。
sLORETAによると$||A^\dagger A||$がソースの分散でした。まずは
$$X = \Sigma A^T(A\Sigma A^T + \lambda ^2 I)^{-1}Y$$
この式の内容から考えていきます。
sLORETAで想定される分散はこうなります。
$$ \Sigma A^T(A\Sigma A^T + \lambda ^2 I)^{-1}A$$
$\Sigma$はソースの分散ですね。
で、$||A^\dagger A||$もソースの分散みたいなものです。
つまり、下記の等式が成立して欲しい。

$$ A^T(A\Sigma A^T + \lambda ^2 C)^{-1}A = \Sigma^2$$
$$\sqrt{A^T(A\Sigma A^T + \lambda ^2 C)^{-1}A} = \Sigma$$
この$\Sigma$を収束させるために、繰り返し上記の式を再帰的に計算してきます。

1. まず、$\Sigma$を計算します。一寸ズレてます。
1. 次に、 $\sqrt{A^T(A\Sigma A^T + \lambda ^2 C)^{-1}A}$を計算します。
1. 2を新しい$\Sigma$として、以下無限ループ。

はい、eLORETAです。一応、eLORETAって探索的に場所を特定していくのに向いているらしいっす。[^eloreta]

### eLORETAの実行
さて、MNEpythonで行う時は、いくらかパラメータを入れる必要があります。
一つはCを決める時のパラメータ、$\epsilon$です。
MNEは$I$に分散を掛け算することでCを求めるらしいのですが、
$\epsilon$も掛け算します。これにより、どの程度正則化するかが決まるようです。
正則化項のコントロール目的のパラメータですね！
それから、max_iterというのがあります。
これは、上記の再帰を何回再帰するかです。
そして、force_equalというのがあります。
これは、脳内の電流は三次元ベクトルですから、それをどう扱うかのパラメータです。
これはdefaultでNoneですが、NoneにしているとMNEがよしなにしてくれます。
looseというパラメータがソース推定であります。
これは脳の皮質の電流の方向のズレをどの程度許容するかのパラメータです。

[^eloreta]: Tamesh Halder, Siddharth Talwar, Amit Kumar Jaiswal and Arpan Banerjee eNeuro 16 July 2019, 6 (4) ENEURO.0170-19.2019

という感じの理解で多分合ってると思うけど、間違ってたらごめん(´・ω・｀)
